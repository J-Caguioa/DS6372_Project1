---
title: "DS 6372 Project 1: [DATASET] Analysis"
author: "Audrene Tiakor, Spencer Fogelman, & Joseph Caguioa"
date: "6/15/2019"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# For used packages, load on startup.
package_list <- c("tidyverse", "gridExtra", "plyr", "corrplot", "car")
lapply(package_list, library, character.only=T)
```

## Introduction

The Microsoft Data Science Capstone dataset was made available on Kaggle in July 2018 (https://www.kaggle.com/nandvard/microsoft-data-science-capstone). Using socioeconomic information aggregated by the United States Department of Agriculture Economic Research Service, the objective is to predict U.S. heart disease mortality (per 100,000 people) at the county level. Considering that heart disease remains the leading global cause of death, investigating risk factors is vital in developing a comprehensive treatment strategy.


## Data Description

For this project, we are working with the microsoft data science capstone data. Within this data set, there are 33 variables, 3 categorical variables, and 30 continuous variables. The categroical variables are: area__rucc, area__urban__influence, and econ__economic_typology. The area__rucc variable which is the Rural-Urban Continuum codes, forming a classification scheme that distinguishes metropolitan counties by degree of urbanization and adjacentty to a metro area. These counties are in the U.S. are assigned one of 9 codes, which can be found from the USDA Economic Research Service, https://www.ers.usda.gov/data-products/rural-urban-continuum-codes/. The area__urban__influence variable represents the urban influence codes. The codes form a classificantion scheme that distinguishes metropolitan counties by means of population size of their metro area, and nonmetropolitan counties by size of the largest city or town and promixity to metro and micropolitan areas. These codes can be found from the USDA Economic Research Service, https://www.ers.usda.gov/data-products/urban-influence-codes/. Last, the econ__economic_typology variable is known as the county typology codes. These codes classify U.S. counties according to six mutulally exclusive categories of economic dependence and six overlapping categories of policy-relevant themes.These codes can be found at USDA Economic Research Service, https://www.ers.usda.gov/data-products/county-typology-codes.aspx. From the continuous variables there are split into different sections, economic: econ__pct_civilian_labor, econ__pct_unemployment, econ__pct_uninsured_adults, econ__pct_uninsured_children, demographics: demo__pct_female, demo__pct_below_18_years_of_age, demo__pct_aged_65_years_and_older, demo__pct_hispanic, demo__pct_non_hispanic_african_american, demo__pct_non_hispanic_white, demo__pct_american_indian_or_alaskan_native, demo__pct_asian, demo__pct_adults_less_than_a_high_school_diploma, demo__pct_adults_with_high_school_diploma, demo__pct_adults_with_some_college, demo__pct_adults_bachelors_or_higher, demo__birth_rate_per_1k, demo__death_rate_per_1k, health: health__pct_adult_obesity, health__pct_adult_smoking, health__pct_diabetes, health__pct_low_birthweight, health__pct_excessive_drinking, health__pct_physical_inacticity, health__air_pollution_particulate_matter, health__homicides_per_100k, health__motor_vehicle_crash_deaths_per_100k, health__pop_per_dentist, health__pop_per_primary_care_physician, and the year which is split into year a and year b.


```{r}
# Load two files, Training_values and Training_labels, as data.frames.
dfValues <- read.csv(file='microsoft-data-science-capstone/Training_values.csv', header=T, sep=",")
dfLabels <- read.csv(file='microsoft-data-science-capstone/Training_labels.csv', header=T, sep=",")

# Add response variable to left side of Training_values and shorten name.
dfFull <- cbind(dfLabels$heart_disease_mortality_per_100k, dfValues)
names(dfFull)[names(dfFull) == 'dfLabels$heart_disease_mortality_per_100k'] <- 'heart_disease_mortality_per_100k'

# Remove row_id.
dfFull <- subset(dfFull, select=-row_id)
```

```{r}
# Optional shorten column names by removing *__ category descriptors.
# names(dfFull) = sub("area__|econ__|demo__|health__", "", names(dfFull))
str(dfFull)
```

* Note that demo__pct_male can be derived as 1 - demo__pct_female.
* Adding all 5 demo__pct_[race] columns produces number between .990 and 1.000, as expected. Race proportions explicitly provided.
* Adding all 4 demo__pct_[education] columns produces number between .990 and 1.000, as expected. Education proportions explicitly provided.
* This dataset is already presplit from a test set. It is unclear whether the available data points are considered repeated measures based on year, as there is no way to identify individual counties.

# Exploratory Data Analysis

For the exploratory data analysis portion of this project, tools such as histograms, heatmaps, and variance of inflation factors will be used. During the exploratory data analysis phase, the distribution of our continuous variables will be address, as well as multicolinearity. Multicolinearity is when there is a presence of very high inter-associations among the explanatory variables(possible predictors). A common occurance of multilinearity can be detected when variables are highly correlated to one another.  The reason why it is important to address the problem of multicolinearity is because if there are high inter-associations among the explanatory vairables, then an statistical inferences made from the data may not be reliable. 

```{r edaHistograms}
# Quick exploratory histograms of any numeric variables.
dfFull %>%
  keep(is.numeric) %>%
  gather() %>%
  ggplot(aes(value)) +
    facet_wrap(~ key, scales="free") +
    geom_histogram() +
    theme(axis.text.x=element_text(size=4, angle=45, vjust=1, hjust=1),
          axis.text.y=element_text(size=6),
          strip.text=element_text(size=4, margin=margin()))

# Quick exploratory histograms of any factors.
dfFull %>%
  keep(is.factor) %>%
  gather() %>%
  ggplot(aes(value)) +
    facet_wrap(~ key, scales="free") +
    geom_bar() +
    theme(axis.text.x=element_text(size=3, angle=45, vjust=1, hjust=1),
          axis.text.y=element_text(size=6),
          strip.text=element_text(size=4, margin=margin()))
```
* I think we should drop the scatterplots and just work with the heatmap and the vif to address multicolinearity. I think that we should do the scatterplots after removing any factors with a high vif, and then doing our model selection.
```{r edaScatterplots}
# Heart disease mortality with factors.
pairs(dfFull[,c(1,2,3,34)])
# Heart disease mortality with employment, insurance, gender, and age variables.
pairs(dfFull[,c(1,5:11)])
# Heart disease mortality with race and birth/death rates.
pairs(dfFull[,c(1,12:16,21:22)])
# Heart disease mortality with education variables.
pairs(dfFull[,c(1,17:20)])
# Heart disease mortality with other health variables.
pairs(dfFull[,c(1,23:28)])
pairs(dfFull[,c(1,29:33)])
```

* Some collinearity shown between race variables because they are proportions.
* Maybe air_pollution_particulate_matter should be redefined as factor.

```{r edaCorrplot(before)}
# Correlation plot only takes numeric data. Remove factor variables.
dfNumeric <- dfFull %>% keep(is.numeric)

# Correlation plot on numeric variables. pairwise.complete.obs ignoring any pairs with NA.
corrplot(cor(dfNumeric, use="pairwise.complete.obs"), type = "upper", tl.col = "black", tl.srt = 45, tl.cex = 0.5, number.cex = 0.3, number.digits=2, method="color", addCoef.col="white")
```

```{r edaVif, eval = FALSE, echo = FALSE}
# VIF function from car library
fullModel <- lm(heart_disease_mortality_per_100k~., data=dfFull, singular.ok=T)
vif(fullModel)[,3]^2
```

An initial attempt at obtaining VIFs results in an error on aliased coefficients, indicating that some variables are likely linearly dependent.

```{r edaVif2}
fullModel <- lm(heart_disease_mortality_per_100k~., data=dfFull, singular.ok=T)
=======
# Check attributes with aliased coefficients.
ld.vars <- attributes(alias(fullModel)$Complete)$dimnames[[1]]
ld.vars
```

According to alias(), a few levels of area__urban_influence and the variable demo__pct_adults_bachelors_or_higher show linear dependence. After dropping these two, VIFs can be obtained from the full model.

```{r edaVif3}
#removing the two variables that show linear dependence 
drop <- c('demo__pct_adults_bachelors_or_higher', 
          'area__urban_influence')
dfPartial <- dfFull[, !(names(dfFull) %in% drop)]
```

```{r edaheatmap(after)}
#using heatmap visual to see which variables have multicolinarity
# Correlation plot only takes numeric data. Remove factor variables.
dfNumeric2 <- dfPartial %>% keep(is.numeric)

# Correlation plot on numeric variables. pairwise.complete.obs ignoring any pairs with NA.
corrplot(cor(dfNumeric2, use="pairwise.complete.obs"), type = "upper", tl.col = "black", tl.srt = 45, tl.cex = 0.5, number.cex = 0.3, number.digits=2, method="color", addCoef.col="white")

```

```{r}
#running vif
=======
# Remove linearly dependent variables.
drop <- c('demo__pct_adults_bachelors_or_higher', 
          'area__urban_influence')
dfPartial <- dfFull[, !(names(dfFull) %in% drop)]

# Build full model with remaining 33 predictors and check VIFs.
>>>>>>> 614d58944d14ddaa0e029963f3e6dc28c9bb3496
=======
# Remove linearly dependent variables.
drop <- c('demo__pct_adults_bachelors_or_higher', 
          'area__urban_influence')
dfPartial <- dfFull[, !(names(dfFull) %in% drop)]

# Build full model with remaining 33 predictors and check VIFs.
>>>>>>> 614d58944d14ddaa0e029963f3e6dc28c9bb3496
fullModel <- lm(heart_disease_mortality_per_100k~., data=dfPartial)
vif(fullModel)[,3]
```

After finding what variables show linear dependence using the alias() function, we removed those variables from our data and proceed to using, the vif function from the car package. When using vif to address colinearity within the data,  the rule of thumb is when a variable has a vif greater than 10, that variable can beremoved from data.Keeping this rul of thumb in mind,  we find two variables that we can remove from our model: demo__pct_non_hispanic_african_american (vif=12.77), and demo__pct_non_hispanic_white (vif=15.70). 

Most of the demographic variables pertaining to race return VIFs greater than 5, namely for non-Hispanic whites (15.70), non-Hispanic African Americans (12.77), Hispanics (10.40), and American Indian or Alaskan Natives (5.81). In each row racial proportions add up to ~1, so it makes sense that these variables are collinear (for example, minority groups generally stay underrepresented in each county). Previous statistics show that mortality rates differ by race, which suggests that including it could add predictive power. However in order to proceed with a reliable model, other factors often correlated with race can remain in its stead.

```{r edaVif4}
# Remove all race-related variables from dataset.
drop2 <- c('demo__pct_hispanic', 
          'demo__pct_non_hispanic_african_american', 
          'demo__pct_non_hispanic_white', 
          'demo__pct_american_indian_or_alaskan_native', 
          'demo__pct_asian')
dfPartial2 <- dfPartial[, !(names(dfPartial) %in% drop2)]

# Build full model with remaining 26 predictors and check VIFs.
fullModel2 <- lm(heart_disease_mortality_per_100k~., data=dfPartial2)
vif(fullModel2)[,3]
```

```{r edaVif5}
par(mfrow=c(2,2))
plot(fullModel)
summary(fullModel)
```


```{r}
#removing demo__pct_non_hispanic_african_american & demo__pct_non_hispanic_white
drop <- c('demo__pct_non_hispanic_african_american', 
          'demo__pct_non_hispanic_white','demo__pct_adults_bachelors_or_higher', 
          'area__urban_influence')
dfPartial2 <- dfFull[, !(names(dfFull) %in% drop)]

```

```{r}
#rerunning vif after removing demo__pct_non_hispanic_african_american & demo__pct_non_hispanic_white
fullModel2 <- lm(heart_disease_mortality_per_100k~., data=dfPartial2)
vif(fullModel2)[,3]

```

After dropping a total of four variables: 'demo__pct_non_hispanic_african_american',  'demo__pct_non_hispanic_white','demo__pct_adults_bachelors_or_higher', and 
'area__urban_influence' the vif factors of the remaining variables are less than 10. We are now ready to proceed with the regression analysis portion of this project. 


# Regression Analysis

## Regression Models: Introduction

The main objective is to build a predictive model for heart disease mortality rate using the provided socioeconomic data.

### Model Selection

```{r}
```

### Checking Assumptions

```{r}
#scatterplot matrix

#possible transformation if any are needed

#plots of the residuals
```

### Comparing Competing Models

```{r}
```

## Parameter Interpretation

## Regression Models: Conclusion

Lorem ipsum dolor sit amet, te his wisi voluptatum. Legimus mentitum senserit ad per. Te cum iudico everti concludaturque, id eam congue primis dolores. Nec at quas augue feugiat, dolor perpetua id sed, ei paulo tritani nec.

-------------------------------

# Secondary Analysis

The full dataset contains information for U.S. counties across two undisclosed years, a and b. However, because the counties themselves are unidentified, it is unclear whether each county is represented by more than one year in the provided training data. (For example, does County X have rows for both Year A and Year B in the original training set? Or is Year A in the training file and Year B in the test file?) Additionally, the span of time between the two years could affect whether one could argue that independence exists between data points. (Across counties, mortality rates may be more similar between 2015 and 2016 than between 2000 and 2016.) In short, true time independence within the data is suspect. 

One could even make a case against spatial independence due to regional elements that might affect known heart disease risk factors, such as statewide smoking regulations or food deserts, but it is assumed that the county classification schema captures these nuances.

Independence is assumed in order to run further analysis. Because each county only has two data points at maximum and it is not possible to match them together, a time series or repeated measures analysis cannot be easily performed. The presence of several factor variables makes a two-way ANOVA the logical next step.

## 2-Way ANOVA: Introduction



## Main Analysis Content

```{r}
```

## 2-Way ANOVA: Conclusion



# Appendix

```{r}
```
